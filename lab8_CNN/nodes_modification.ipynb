{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 8.1: nodes modification\n",
    "by Domrachev Ivan, B20-Ro-01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-13 23:41:48.794824: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-11-13 23:41:48.844324: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-11-13 23:41:48.845027: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-11-13 23:41:49.840787: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from nn_from_scratch.neurons import Convolution\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1. Bathched & Biased Convolution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The convolutional layer was extended to support batches inputs and optional bias input\n",
    "> Note: the solution is far from being generalized, f.e. it lacks padding, stride settings, as well as support of batches of the pictures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = (10, 3, 7, 5)\n",
    "kernel_size = 2\n",
    "output_layers = 20\n",
    "conv = Convolution(input_dim, kernel_size, output_layers=output_layers, use_bias=False)\n",
    "output_dim = conv._output_dim\n",
    "\n",
    "# Random input values (x itself, and assumed partial derivative)\n",
    "x_input = np.random.random(input_dim).astype(dtype=np.float32)\n",
    "dL_dy = np.random.random(output_dim).astype(dtype=np.float32)\n",
    "\n",
    "output = conv.forward(x_input)\n",
    "dL_dx = conv.backward(dL_dy)\n",
    "dL_dw = conv._W_pd\n",
    "# bias = conv._B\n",
    "# dL_db = conv._B_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10, 20, 6, 4), (20, 3, 2, 2), (10, 3, 7, 5))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape, conv._W.shape, x_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "b, m, n, p, q = input_dim[0], input_dim[2], input_dim[3], kernel_size, kernel_size\n",
    "assert all(\n",
    "    np.allclose(\n",
    "        output[i][kern][j][k], (x_input[i, :, j:j+p, k:k+q] * conv._W[kern]).sum()\n",
    "    ) \n",
    "    for i in range(b) \n",
    "    for j in range(m-p+1) \n",
    "    for k in range(n-q+1)\n",
    "    for kern in range(output_layers)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, let's compare its performance with tensorflow implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 3, 2, 2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv.W.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-13 23:41:51.534355: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-11-13 23:41:51.535228: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1960] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "x_input_batched = tf.constant(\n",
    "    np.moveaxis(\n",
    "        x_input,\n",
    "        1, -1\n",
    "    ), \n",
    "    dtype=tf.float32\n",
    ")\n",
    "\n",
    "weights_reshaped = tf.constant(\n",
    "    conv.W.transpose(2, 3, 1, 0),   \n",
    "    dtype=tf.float32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_keras = layers.Conv2D(\n",
    "    20, 2,\n",
    "    input_shape=x_input_batched.shape[1:],\n",
    "    use_bias=False,\n",
    "    kernel_initializer=tf.keras.initializers.Constant(weights_reshaped),\n",
    "    data_format='channels_last'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([10, 7, 5, 3])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_input_batched.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.GradientTape(persistent=True) as tape:\n",
    "    tape.watch(x_input_batched)  # Watch the input tensor for gradient computation\n",
    "    conv_output = conv_keras(x_input_batched)\n",
    "\n",
    "conv_output_np = conv_output.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 6, 4, 20)\n",
      "(10, 20, 6, 4)\n"
     ]
    }
   ],
   "source": [
    "print(conv_output_np.shape)\n",
    "conv_output_np = np.moveaxis(conv_output_np, -1, 1)\n",
    "print(conv_output_np.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 20, 6, 4)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Check that layer output is correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.04081991,  0.00420363,  0.09719883, -0.00735995],\n",
       "       [ 0.06143697,  0.07457685,  0.00036471,  0.07781924],\n",
       "       [ 0.00496632,  0.04221429,  0.06450655, -0.00169559],\n",
       "       [ 0.00320771,  0.03251834, -0.03337364,  0.08637109],\n",
       "       [ 0.00873353, -0.00916717,  0.12397933, -0.0313285 ],\n",
       "       [-0.00855705,  0.0378824 ,  0.00410559,  0.16866183]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.04081991,  0.00420364,  0.09719883, -0.00735995],\n",
       "       [ 0.06143697,  0.07457686,  0.00036472,  0.07781925],\n",
       "       [ 0.00496632,  0.04221429,  0.06450653, -0.00169559],\n",
       "       [ 0.00320773,  0.03251833, -0.03337364,  0.0863711 ],\n",
       "       [ 0.00873353, -0.00916717,  0.12397933, -0.0313285 ],\n",
       "       [-0.00855705,  0.0378824 ,  0.00410559,  0.16866183]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_output_np[0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(np.abs(conv_output_np[:, :, :, :] - output[:, :, :, :])) < 1e-6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Check that backpropogation for both input and weights is correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dL_dy_keras = tf.constant(np.moveaxis(dL_dy, 1, -1), dtype=tf.float32)\n",
    "\n",
    "dL_dx_keras = tape.gradient(\n",
    "    conv_output, x_input_batched, output_gradients=dL_dy_keras\n",
    ")\n",
    "dL_dw_keras = tape.gradient(\n",
    "    conv_output, conv_keras.trainable_variables, output_gradients=dL_dy_keras\n",
    ")\n",
    "\n",
    "dL_dx_keras_np = np.moveaxis(dL_dx_keras.numpy().squeeze(), -1, 1)\n",
    "dL_dw_keras_np = np.moveaxis(dL_dw_keras[0].numpy().squeeze(), -1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10, 3, 7, 5), (10, 3, 7, 5))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dL_dx_keras_np.shape, dL_dx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(np.abs(dL_dx_keras_np - dL_dx)) < 1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(np.abs(dL_dw_keras_np - np.moveaxis(dL_dw, 1, -1))) < 1e-5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support for tensor inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's quite trivial to add 4D tensor support to ReLU, but anyway, let's validate it's functionality:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = (5, 3, 10, 10)\n",
    "conv = Convolution(input_dim, kernel_size, output_layers=output_layers)\n",
    "output_dim = conv._output_dim\n",
    "\n",
    "# Random input values (x itself, and assumed partial derivative)\n",
    "x_input = np.random.random(input_dim)\n",
    "dL_dy = np.random.random(output_dim)\n",
    "\n",
    "output = conv.forward(x_input)\n",
    "dL_dx = conv.backward(dL_dy)\n",
    "dL_dw = conv._W_pd\n",
    "bias = conv._B\n",
    "dL_db = conv._B_pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[-10   7]\n",
      "   [ -1   4]]\n",
      "\n",
      "  [[  2   4]\n",
      "   [ -2  -4]]]\n",
      "\n",
      "\n",
      " [[[  7   6]\n",
      "   [  4  -4]]\n",
      "\n",
      "  [[ -9  -1]\n",
      "   [  1  -2]]]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[[[0, 7],\n",
       "          [0, 4]],\n",
       " \n",
       "         [[2, 4],\n",
       "          [0, 0]]],\n",
       " \n",
       " \n",
       "        [[[7, 6],\n",
       "          [4, 0]],\n",
       " \n",
       "         [[0, 0],\n",
       "          [1, 0]]]]),\n",
       " array([[[[ 0, -2],\n",
       "          [ 0,  5]],\n",
       " \n",
       "         [[-2, -9],\n",
       "          [ 0,  0]]],\n",
       " \n",
       " \n",
       "        [[[-5, -5],\n",
       "          [-9,  0]],\n",
       " \n",
       "         [[ 0,  0],\n",
       "          [ 2,  0]]]]))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nn_from_scratch.nodes import ReLU  \n",
    "import numpy as np\n",
    "\n",
    "input_dim = (2, 2, 2, 2)\n",
    "\n",
    "# Random input values (x itself, and assumed partial derivative)\n",
    "x_input = np.random.randint(-10, 10, size=input_dim)\n",
    "dL_dy = np.random.randint(-10, 10, size=input_dim)\n",
    "print(x_input)\n",
    "relu = ReLU(input_dim)\n",
    "\n",
    "# Forward call\n",
    "y_value = relu.forward(x_input)\n",
    "\n",
    "# Backpropogation\n",
    "dL_dx = relu.backward(dL_dy)\n",
    "\n",
    "y_value, dL_dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As one could see, the functionality works"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another handy node is Vectorizer -- it transforms everything into vector. It's very handy, since it allows to utilize existing code for CNN as well!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2, 8), (2, 2, 2, 2))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nn_from_scratch.nodes import Vectorization  \n",
    "import numpy as np\n",
    "\n",
    "input_dim = (2, 2, 2, 2)\n",
    "\n",
    "# Random input values (x itself, and assumed partial derivative)\n",
    "x_input = np.random.randint(-10, 10, size=input_dim)\n",
    "\n",
    "vectorize = Vectorization(input_dim)\n",
    "output_dim = vectorize._output_dim\n",
    "dL_dy = np.random.randint(-10, 10, size=output_dim)\n",
    "\n",
    "# Forward call\n",
    "y_value = vectorize(x_input)\n",
    "\n",
    "# Backpropogation\n",
    "dL_dx = vectorize.backward(dL_dy)\n",
    "\n",
    "y_value.shape, dL_dx.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_courses",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
