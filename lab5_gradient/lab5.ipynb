{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 5. Vector and Matrix Backpropogation\n",
    "by Domrachev Ivan, B20-Ro-01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nn_from_scratch.nodes import SoftMax, ReLU\n",
    "from nn_from_scratch.neurons import Linear\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1. Vectorized softmax function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note: accidentally, I've implemented this functionality in the previous lab assignment. Here is the copy of corresponding charpter from there"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the logic is implemented in the file [backprop.py](./backprop.py). There is an abstract class Node, which implements all the expected logic except for the function itself and the jacobian matrix calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.01165623, 0.03168492, 0.08612854, 0.23412166, 0.63640865]),\n",
       " array([-0.00510617,  0.01780491,  0.1345273 ,  0.13156147, -0.27878751]))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm = SoftMax(5)\n",
    "\n",
    "# Random input values (x itself, and assumed partial derivative)\n",
    "x_input = np.array([0, 1, 2, 3, 4])\n",
    "dL_dy = np.array([1, 2, 3, 2, 1])\n",
    "\n",
    "# Forward call\n",
    "y_value = sm.forward(x_input)\n",
    "\n",
    "# Backpropogation\n",
    "dL_dx = sm.backward(dL_dy)\n",
    "\n",
    "y_value, dL_dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The verification was described in the previous lab, the code (as well as outputs) haven't changed, so let's omit that here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2. ReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function ReLU, as well as the partial derivative, were implemented according to the lecture. Note that despite derivative $\\frac{\\partial ReLU}{\\partial x} (0)$ was chosen, it was assigned to $0$ in my implementation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 0, 2, 0, 4]), array([0, 0, 3, 0, 1]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relu = ReLU(5)\n",
    "\n",
    "# Random input values (x itself, and assumed partial derivative)\n",
    "x_input = np.array([0, -1, 2, -1, 4])\n",
    "dL_dy = np.array([1, 2, 3, 2, 1])\n",
    "\n",
    "# Forward call\n",
    "y_value = relu.forward(x_input)\n",
    "\n",
    "# Backpropogation\n",
    "dL_dx = relu.backward(dL_dy)\n",
    "\n",
    "y_value, dL_dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is easy to see from that example that `ReLU` function works correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, `ReLU` supports matrix input. In this case, it applies `ReLU` functoin row-wise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0, 0, 2, 0, 4],\n",
       "        [0, 0, 2, 0, 4]]),\n",
       " array([[0, 0, 3, 0, 1],\n",
       "        [0, 0, 3, 0, 1]]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relu = ReLU((2, 5))\n",
    "\n",
    "# Random input values (x itself, and assumed partial derivative)\n",
    "x_input = np.array([\n",
    "    [0, -1, 2, -1, 4],\n",
    "    [0, -1, 2, -1, 4]\n",
    "])\n",
    "dL_dy = np.array([\n",
    "    [1, 2, 3, 2, 1], \n",
    "    [1, 2, 3, 2, 1]\n",
    "])\n",
    "\n",
    "# Forward call\n",
    "y_value = relu.forward(x_input)\n",
    "\n",
    "# Backpropogation\n",
    "dL_dx = relu.backward(dL_dy)\n",
    "\n",
    "y_value, dL_dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3. Linear layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *Note. Due to structure of the code, the function $Y = XW$ would be implemented*\n",
    "\n",
    "To implement linear node $Y = XW$ in my structure, I decided to make the following assumptions:\n",
    "1. The weigts matrix `W` is an inner parameter of the class, which could be accessed via a getter. It's partial derivative is also stored inside it.\n",
    "2. Despite there are functions to compute derivatives w.r.t. `W` and `X`, I decided to override the functoin `forward` with dot products from `numpy`, because this approach is more efficient\n",
    "\n",
    "You can explore the implementation in `backprop.py` file.\n",
    "> *Note. To compute the jacobian $\\frac{\\partial Y}{\\partial X}$ and $\\frac{\\partial Y}{\\partial W}$, the key note is that:*\n",
    "> 1. *$\\frac{\\partial Y}{\\partial X}_{i, j}$ is a zero matrix, except for the $j$-th column, which equals to:*\n",
    "> $$\\frac{\\partial Y}{\\partial X}_{i, j, l, j} = W_{l, j}$$\n",
    "> 2. *Similarly, $\\frac{\\partial Y}{\\partial W}_{i, j}$ is a zero matrix, except for the $i$-th raw, which equals to:*\n",
    "> $$\\frac{\\partial Y}{\\partial W}_{i, j, j, l} = X_{j, l}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward output:\n",
      "[[0.80514121 0.978518   0.84292487 0.86549299 1.03282263]\n",
      " [0.91727081 1.0731285  0.96631008 0.97080347 1.12494457]\n",
      " [0.77990627 0.90017716 0.8277365  0.82073204 0.93491457]]\n",
      "Jacobian dL/dx:\n",
      "[[1.68644384 1.77122432]\n",
      " [1.26119332 1.4110765 ]\n",
      " [0.85105349 0.93005214]]\n",
      "Jacobian dL/dw:\n",
      "[[1.37818991 2.31176235 0.98901982 1.64949482 1.93039748]\n",
      " [0.51568113 1.19270197 0.286197   0.70855942 0.7288801 ]\n",
      " [0.73371861 0.94336793 0.59995179 0.78918571 1.12469091]]\n"
     ]
    }
   ],
   "source": [
    "input_dim=(3, 2)\n",
    "output_dim=(3, 5)\n",
    "linear = Linear(input_dim, output_dim)\n",
    "\n",
    "# Random input values (x itself, and assumed partial derivative)\n",
    "x_input = np.random.rand(*input_dim)\n",
    "dL_dy = np.random.rand(*output_dim)\n",
    "\n",
    "# Forward call\n",
    "y_value = linear.forward(x_input)\n",
    "\n",
    "# Backpropogation\n",
    "dL_dx = linear.backward(dL_dy)\n",
    "dL_dw = linear._W_pd\n",
    "\n",
    "print(\"Forward output:\")\n",
    "print(y_value)\n",
    "print(\"Jacobian dL/dx:\")\n",
    "print(dL_dx)\n",
    "print(\"Jacobian dL/dw:\")\n",
    "print(dL_dw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also note that calculations of back propogation wia jacobians lead to the same result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jacobian dL/dx:\n",
      "[[1.68644384 1.77122432]\n",
      " [1.26119332 1.4110765 ]\n",
      " [0.85105349 0.93005214]]\n",
      "Jacobian dL/dw\n",
      "[[1.37818991 2.31176235 0.98901982 1.64949482 1.93039748]\n",
      " [0.51568113 1.19270197 0.286197   0.70855942 0.7288801 ]\n",
      " [0.73371861 0.94336793 0.59995179 0.78918571 1.12469091]]\n"
     ]
    }
   ],
   "source": [
    "dL_dx_jac = linear.backward(dL_dy, use_jacobian=True)\n",
    "dL_dw_jac = linear._W_pd\n",
    "\n",
    "print(\"Jacobian dL/dx:\")\n",
    "print(dL_dx_jac)\n",
    "\n",
    "print(\"Jacobian dL/dw\")\n",
    "print(dL_dw_jac)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Linear` also accepts vector inputs, automatically casting them to matrix `(1, n_input)` shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward output:\n",
      "[1.67283515 1.88932967 1.69074605 1.82291818 1.8870572 ]\n",
      "Jacobian dL/dx:\n",
      "[1.277602   1.38791781 1.44945885]\n",
      "Jacobian dL/dw:\n",
      "[[0.39846947 0.6232179  0.92781628 0.2222169  0.52429699]\n",
      " [0.39761295 0.62187827 0.92582189 0.22173923 0.52316999]\n",
      " [0.34877575 0.54549546 0.81210692 0.19450389 0.45891113]\n",
      " [0.27312598 0.42717701 0.63596021 0.15231583 0.35937289]]\n"
     ]
    }
   ],
   "source": [
    "input_dim = 3\n",
    "output_dim = 5\n",
    "linear = Linear(input_dim, output_dim)\n",
    "\n",
    "# Random input values (x itself, and assumed partial derivative)\n",
    "x_input = np.random.rand(input_dim)\n",
    "dL_dy = np.random.rand(output_dim)\n",
    "\n",
    "# Forward call\n",
    "y_value = linear.forward(x_input)\n",
    "\n",
    "# Backpropogation\n",
    "dL_dx = linear.backward(dL_dy)\n",
    "dL_dw = linear._W_pd\n",
    "\n",
    "print(\"Forward output:\")\n",
    "print(y_value)\n",
    "print(\"Jacobian dL/dx:\")\n",
    "print(dL_dx)\n",
    "print(\"Jacobian dL/dw:\")\n",
    "print(dL_dw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also note that calculations of back propogation wia jacobians lead to the same result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jacobian dL/dx:\n",
      "[1.277602   1.38791781 1.44945885]\n",
      "Jacobian dL/dw\n",
      "[[0.39846947 0.6232179  0.92781628 0.2222169  0.52429699]\n",
      " [0.39761295 0.62187827 0.92582189 0.22173923 0.52316999]\n",
      " [0.34877575 0.54549546 0.81210692 0.19450389 0.45891113]\n",
      " [0.27312598 0.42717701 0.63596021 0.15231583 0.35937289]]\n"
     ]
    }
   ],
   "source": [
    "dL_dx_jac = linear.backward(dL_dy, use_jacobian=True)\n",
    "dL_dw_jac = linear._W_pd\n",
    "\n",
    "print(\"Jacobian dL/dx:\")\n",
    "print(dL_dx_jac)\n",
    "\n",
    "print(\"Jacobian dL/dw\")\n",
    "print(dL_dw_jac)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_courses",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
