{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 8, part 2: Convolutional Neural Network sample\n",
    "by Domrachev Ivan, B20-Ro-01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-10 17:18:34.833098: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-11-10 17:18:34.901871: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-11-10 17:18:34.902703: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-11-10 17:18:36.344581: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "from nn_from_scratch.optimizers import GradientDescent\n",
    "from nn_from_scratch.examples.simple_nn import NeuralNetwork\n",
    "from nn_from_scratch.nodes import ReLU, SoftMaxLoss, Vectorization\n",
    "from nn_from_scratch.neurons import Linear, Convolution\n",
    "from nn_from_scratch.interfaces import Neuron\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers as t_layers\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1. One layer NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, everything is ready to create a Convolutional Neural Network!\n",
    "\n",
    "Conviniently, the framework from the simple Neural Network is suitable for the CNN as well, so let's utilize it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_X, train_y), (test_X, test_y) = mnist.load_data()\n",
    "# CNN are slow...\n",
    "train_X = train_X[:1000]\n",
    "n_input, n_output, batch_size, n_channels = 28**2, 10, 50, 1\n",
    "assert train_X.shape[0] % batch_size == 0\n",
    "\n",
    "train_X = train_X.reshape(train_X.shape[0] // batch_size, batch_size, n_channels, 28, 28)\n",
    "train_y_ohe = np.zeros((len(train_y), n_output))\n",
    "train_y_ohe[np.arange(len(train_y)), train_y] = 1\n",
    "train_y_ohe = train_y_ohe.reshape((train_y.shape[0] // batch_size, batch_size, n_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The values preferably should belong to $[0; 1]$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = train_X / 255\n",
    "test_X = test_X / 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [01:43<00:00,  5.18s/it, loss=2.34]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 2.3388391875914034\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [01:51<00:00,  5.58s/it, loss=2.33]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Loss: 2.3270086079741086\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([2.32700861])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gds = GradientDescent(lr=0.5)\n",
    "layers = [\n",
    "    Convolution(\n",
    "        (batch_size, n_channels, 28, 28), \n",
    "        kernel_size=3, \n",
    "        output_layers=4,\n",
    "        use_bias=False\n",
    "    ),\n",
    "    ReLU((batch_size, 4, 26, 26)),\n",
    "    Convolution(\n",
    "        (batch_size, 4, 26, 26), \n",
    "        kernel_size=7,\n",
    "        output_layers=2,\n",
    "        use_bias=False\n",
    "    ),\n",
    "    ReLU((batch_size, 2, 20, 20)),\n",
    "    Vectorization((batch_size, 2, 20, 20)),\n",
    "    Linear((batch_size, 800), (batch_size, n_output))\n",
    "]\n",
    "loss_fn = SoftMaxLoss((batch_size, n_output))\n",
    "network = NeuralNetwork(\n",
    "    n_input=n_input, \n",
    "    n_output=n_output, \n",
    "    batch_size=batch_size, \n",
    "    optimizer=gds,\n",
    "    layers=layers,  \n",
    "    loss_fn=loss_fn\n",
    ")\n",
    "network.fit(train_X, train_y_ohe, n_epochs=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, loss is decreasing very slowly. Strange, let's check the predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(test_X: np.array, test_y: np.array, model) -> float:\n",
    "    correct_predictions = 0\n",
    "    total = 0\n",
    "\n",
    "    for test_inputs, test_labels in zip(test_X, test_y):\n",
    "        predicts = model.predict(test_inputs)\n",
    "        pred_class = np.argmax(predicts, axis=1)\n",
    "\n",
    "        correct_predictions += (pred_class == test_labels).sum()\n",
    "        total += len(test_labels)\n",
    "\n",
    "    return correct_predictions / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.1135\n"
     ]
    }
   ],
   "source": [
    "test_X_batches = test_X.reshape((test_X.shape[0] // batch_size, batch_size, 28, 28))\n",
    "test_y_batches = test_y.reshape((test_y.shape[0] // batch_size, batch_size,))\n",
    "\n",
    "acc = compute_accuracy(test_X_batches, test_y_batches, network)\n",
    "print(f\"Accuracy: {acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, they are random... Maybe the model itself is invalid, let's train tensorflow analogue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2. Learning TensorFlow network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part is straight-forward, would not explain much:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_X, train_y), (test_X, test_y) = mnist.load_data()\n",
    "train_X = np.expand_dims(train_X / 255, axis=-1)\n",
    "test_X = np.expand_dims(test_X / 255, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_8 (Conv2D)           (None, 26, 26, 4)         36        \n",
      "                                                                 \n",
      " re_lu_8 (ReLU)              (None, 26, 26, 4)         0         \n",
      "                                                                 \n",
      " conv2d_9 (Conv2D)           (None, 20, 20, 2)         392       \n",
      "                                                                 \n",
      " re_lu_9 (ReLU)              (None, 20, 20, 2)         0         \n",
      "                                                                 \n",
      " flatten_4 (Flatten)         (None, 800)               0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 10)                8010      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 8438 (32.96 KB)\n",
      "Trainable params: 8438 (32.96 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "tf_layers = [\n",
    "    t_layers.Conv2D(\n",
    "        4, 3,\n",
    "        input_shape=train_X.shape[1:],\n",
    "        use_bias=False,\n",
    "    ),\n",
    "    t_layers.ReLU(),\n",
    "    t_layers.Conv2D(\n",
    "        2, 7,\n",
    "        use_bias=False,\n",
    "    ),\n",
    "    t_layers.ReLU(),\n",
    "    t_layers.Flatten(),\n",
    "    t_layers.Dense(\n",
    "        10,\n",
    "        use_bias=True,\n",
    "    )\n",
    "]\n",
    "model = tf.keras.models.Sequential()\n",
    "for l in tf_layers:\n",
    "    model.add(l)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.4913 - accuracy: 0.8509 - val_loss: 0.2747 - val_accuracy: 0.9211\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 25s 13ms/step - loss: 0.2312 - accuracy: 0.9323 - val_loss: 0.1786 - val_accuracy: 0.9491\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 29s 15ms/step - loss: 0.1694 - accuracy: 0.9507 - val_loss: 0.1412 - val_accuracy: 0.9581\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 28s 15ms/step - loss: 0.1470 - accuracy: 0.9574 - val_loss: 0.1358 - val_accuracy: 0.9577\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 30s 16ms/step - loss: 0.1341 - accuracy: 0.9609 - val_loss: 0.1144 - val_accuracy: 0.9641\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='sgd',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(train_X, train_y, epochs=5, \n",
    "                    validation_data=(test_X, test_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy there is not random, and the network performs great. Maybe the weights are different?.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3. Comparison with TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should compare:\n",
    "1. Outputs of the model (given the same initial weights and the same input)\n",
    "2. Partial derivative of loss w.r.t. input. If they are close enough, then all the inner states are similar as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_pic, example_label = train_X[0], train_y_ohe[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = example_pic.copy()\n",
    "for layer in layers:\n",
    "    state = layer.forward(state)\n",
    "\n",
    "loss = loss_fn.forward(state, example_label)\n",
    "dL_dy = loss_fn.backward()\n",
    "partial_derivative = dL_dy\n",
    "dL_dx = []\n",
    "dL_dw = []\n",
    "\n",
    "for layer in layers[::-1]:\n",
    "    partial_derivative = layer.backward(partial_derivative)\n",
    "    dL_dx.append(partial_derivative)\n",
    "\n",
    "    if isinstance(layer, Neuron):\n",
    "        dL_dw.append(layer._W_pd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 1, 28, 28)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_pic.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-07 14:16:17.564457: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-11-07 14:16:17.565690: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1960] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "example_pic_tf = tf.constant(\n",
    "    np.moveaxis(\n",
    "        example_pic,\n",
    "        1, -1\n",
    "    ), \n",
    "    dtype=tf.float32\n",
    ")\n",
    "\n",
    "weights_tf = [\n",
    "    tf.constant(\n",
    "        np.moveaxis(\n",
    "            l.W,\n",
    "            0, -1\n",
    "        ).astype(np.float32)\n",
    "    )\n",
    "    for l in [layers[0], layers[2]]\n",
    "]\n",
    "weights_tf.extend([\n",
    "    tf.constant(\n",
    "        np.moveaxis(\n",
    "            layers[5].W[1:, :],\n",
    "            0, -1\n",
    "        ).astype(np.float32)\n",
    "    ),\n",
    "    tf.constant(\n",
    "        np.moveaxis(\n",
    "            layers[5].W[0, :],\n",
    "            0, -1\n",
    "        ).astype(np.float32)\n",
    "    ),  \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([50, 28, 28, 1])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_pic_tf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 26, 26, 4)         36        \n",
      "                                                                 \n",
      " re_lu (ReLU)                (None, 26, 26, 4)         0         \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 20, 20, 2)         392       \n",
      "                                                                 \n",
      " re_lu_1 (ReLU)              (None, 20, 20, 2)         0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 800)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                8010      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 8438 (32.96 KB)\n",
      "Trainable params: 8438 (32.96 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "tf_layers = [\n",
    "    t_layers.Conv2D(\n",
    "        4, 3,\n",
    "        input_shape=example_pic_tf.shape[1:],\n",
    "        use_bias=False,\n",
    "        kernel_initializer=tf.keras.initializers.Constant(weights_tf[0])\n",
    "    ),\n",
    "    t_layers.ReLU(),\n",
    "    t_layers.Conv2D(\n",
    "        2, 7,\n",
    "        use_bias=False,\n",
    "        kernel_initializer=tf.keras.initializers.Constant(weights_tf[1])\n",
    "    ),\n",
    "    t_layers.ReLU(),\n",
    "    t_layers.Flatten(),\n",
    "    t_layers.Dense(\n",
    "        10,\n",
    "        use_bias=True,\n",
    "        kernel_initializer=tf.keras.initializers.Constant(weights_tf[2]),\n",
    "        bias_initializer=tf.keras.initializers.Constant(weights_tf[3])\n",
    "    )\n",
    "]\n",
    "model = tf.keras.models.Sequential()\n",
    "for l in tf_layers:\n",
    "    model.add(l)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.GradientTape(persistent=True) as tape:\n",
    "    tape.watch(example_pic_tf)\n",
    "    conv_output = model(example_pic_tf)\n",
    "\n",
    "conv_output_np = conv_output.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. The outputs are similar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max((conv_output_np - state[0]) < 2e-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dL_dy_keras = tf.constant(np.moveaxis(dL_dy, 1, -1), dtype=tf.float32)\n",
    "\n",
    "dL_dx_keras = tape.gradient(conv_output, example_pic_tf, output_gradients=dL_dy_keras)\n",
    "\n",
    "dL_dw_keras = [\n",
    "    tape.gradient(conv_output, layer_i.trainable_variables)\n",
    "    for layer_i in [tf_layers[0], tf_layers[2], tf_layers[5]]\n",
    "]\n",
    "\n",
    "dL_dx_keras_np = np.moveaxis(dL_dx_keras.numpy(), -1, 1)\n",
    "dL_dw_keras_np = [dL_dw_keras_i[0].numpy() for dL_dw_keras_i in dL_dw_keras][::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dL_dw[0] = dL_dw[0][1:, :]\n",
    "dL_dw[1] = dL_dw[1].T\n",
    "dL_dw[2] = dL_dw[2].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(800, 10)\n",
      "(7, 7, 4, 2)\n",
      "(3, 3, 1, 4)\n"
     ]
    }
   ],
   "source": [
    "for a in dL_dw_keras_np:\n",
    "    print(a.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. The output of the backpropogation is similar (hence, all the inner states of back propogation are similar as well)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.all((dL_dx_keras_np - dL_dx[-1]) < 1e-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have several ideas why results diverge and will try to fix them in future works."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_courses",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
