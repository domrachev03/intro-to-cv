{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3. Parametric approach\n",
    "by Domrachev Ivan, B20-Ro-01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1. Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the dataset is loaded from the torchflow, let's convert it to numpy array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((50000, 3072), (50000,))"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cifar10_data_np = trainset.data\n",
    "n_samples, w, h, n_colors = cifar10_data_np.shape\n",
    "\n",
    "cifar10_data_np = (\n",
    "    cifar10_data_np.reshape(n_samples, -1) / 256\n",
    ").astype(np.float64)\n",
    "\n",
    "cifar10_labels_np = np.array(trainset.targets)\n",
    "n_classes = len(trainset.classes)\n",
    "\n",
    "cifar10_data_np.shape, cifar10_labels_np.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's split it onto train and validation datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train = int(0.8 * n_samples)\n",
    "n_val = n_samples - n_train\n",
    "\n",
    "cifar10_train_data = cifar10_data_np[:n_train]\n",
    "cifar10_val_data = cifar10_data_np[n_train:]\n",
    "\n",
    "cifar10_train_labels = cifar10_labels_np[:n_train]\n",
    "cifar10_val_labels = cifar10_labels_np[n_train:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell contains all the required functions. Some of them are modified to work not only elementwise, but also for the arrays of multiple samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def labels_encoding(labels: np.ndarray) -> np.ndarray:\n",
    "    '''One Hot Encoding of the labels'''\n",
    "\n",
    "    n_classes = len(np.unique(labels))\n",
    "    encoded = np.zeros((labels.shape[0], n_classes))\n",
    "\n",
    "    for i in range(labels.shape[0]):\n",
    "        encoded[i, labels] = 1\n",
    "    \n",
    "    return encoded\n",
    "\n",
    "def f (x: np.ndarray, W: np.ndarray) -> np.ndarray:\n",
    "    '''Parametric predictor, y_hat = W @ x'''\n",
    "    \n",
    "    return (W @ x.T).T\n",
    "\n",
    "\n",
    "def cross_entropy_loss(y_hat: np.ndarray, y: np.ndarray) -> np.ndarray:\n",
    "    '''Cross Entropy loss function'''\n",
    "    \n",
    "    n = len(y)\n",
    "    l = 0\n",
    "    for i in range(n):\n",
    "        l = (l - np.dot(y[i], np.log(y_hat[i]))) \n",
    "    return l / n\n",
    "\n",
    "\n",
    "def softmax(y: np.ndarray) -> np.ndarray:\n",
    "    '''Softmax function'''\n",
    "    return (np.exp(y).T / np.exp(y).sum(axis=1)).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final part is to find the optimal matrix W, and validate its accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_W, min_loss = None, float('inf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar10_labels_encoded = labels_encoding(cifar10_train_labels)\n",
    "\n",
    "n = 10**3\n",
    "for _ in range(n):\n",
    "    W = np.random.rand(n_classes, w*h*n_colors) / 100\n",
    "    y_hat = softmax(f(cifar10_train_data, W))\n",
    "    loss = cross_entropy_loss(y_hat, cifar10_labels_encoded)\n",
    "    if loss < min_loss:\n",
    "        min_loss = loss\n",
    "        best_W = W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is 1108/10000, 0.1108%\n"
     ]
    }
   ],
   "source": [
    "preds = softmax(f(cifar10_val_data, best_W))\n",
    "correct_preds = (np.argmax(preds, axis=1) == cifar10_val_labels).sum()\n",
    "\n",
    "print(f'Accuracy is {correct_preds}/{n_val}, {correct_preds/n_val}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_courses",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
