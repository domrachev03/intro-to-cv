{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 4. Backpropogation\n",
    "by Domrachev Ivan, B20-Ro-01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nn_from_scratch.nodes import SoftMax, NormalizedSoftMax\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1. Softmax function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the logic is implemented in the file [backprop.py](./backprop.py). There is an abstract class Node, which implements all the expected logic except for the function itself and the jacobian matrix calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.01165623, 0.03168492, 0.08612854, 0.23412166, 0.63640865]),\n",
       " array([-0.00510617,  0.01780491,  0.1345273 ,  0.13156147, -0.27878751]))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm = SoftMax(5)\n",
    "\n",
    "# Random input values (x itself, and assumed partial derivative)\n",
    "x_input = np.array([0, 1, 2, 3, 4])\n",
    "dL_dy = np.array([1, 2, 3, 2, 1])\n",
    "\n",
    "# Forward call\n",
    "y_value = sm.forward(x_input)\n",
    "\n",
    "# Backpropogation\n",
    "dL_dx = sm.backward(dL_dy)\n",
    "\n",
    "y_value, dL_dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To validate the result, let's use the `sympy` for a numerical calculations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\left[\\begin{matrix}\\frac{e^{x_{0}}}{e^{x_{0}} + e^{x_{1}} + e^{x_{2}} + e^{x_{3}} + e^{x_{4}}}\\\\\\frac{e^{x_{1}}}{e^{x_{0}} + e^{x_{1}} + e^{x_{2}} + e^{x_{3}} + e^{x_{4}}}\\\\\\frac{e^{x_{2}}}{e^{x_{0}} + e^{x_{1}} + e^{x_{2}} + e^{x_{3}} + e^{x_{4}}}\\\\\\frac{e^{x_{3}}}{e^{x_{0}} + e^{x_{1}} + e^{x_{2}} + e^{x_{3}} + e^{x_{4}}}\\\\\\frac{e^{x_{4}}}{e^{x_{0}} + e^{x_{1}} + e^{x_{2}} + e^{x_{3}} + e^{x_{4}}}\\end{matrix}\\right]$"
      ],
      "text/plain": [
       "Matrix([\n",
       "[exp(x_0)/(exp(x_0) + exp(x_1) + exp(x_2) + exp(x_3) + exp(x_4))],\n",
       "[exp(x_1)/(exp(x_0) + exp(x_1) + exp(x_2) + exp(x_3) + exp(x_4))],\n",
       "[exp(x_2)/(exp(x_0) + exp(x_1) + exp(x_2) + exp(x_3) + exp(x_4))],\n",
       "[exp(x_3)/(exp(x_0) + exp(x_1) + exp(x_2) + exp(x_3) + exp(x_4))],\n",
       "[exp(x_4)/(exp(x_0) + exp(x_1) + exp(x_2) + exp(x_3) + exp(x_4))]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sympy as sp \n",
    "\n",
    "n = 5\n",
    "x = sp.symbols(' '.join([f'x_{i}' for i in range(n)]))\n",
    "exp_x = sp.Matrix([\n",
    "    sp.exp(x_i) for x_i in x\n",
    "])\n",
    "exp_sum = sum(exp_x)\n",
    "\n",
    "softmax = exp_x / exp_sum\n",
    "softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\left[\\begin{matrix}\\frac{e^{x_{0}}}{e^{x_{0}} + e^{x_{1}} + e^{x_{2}} + e^{x_{3}} + e^{x_{4}}} - \\frac{e^{2 x_{0}}}{\\left(e^{x_{0}} + e^{x_{1}} + e^{x_{2}} + e^{x_{3}} + e^{x_{4}}\\right)^{2}} & - \\frac{e^{x_{0}} e^{x_{1}}}{\\left(e^{x_{0}} + e^{x_{1}} + e^{x_{2}} + e^{x_{3}} + e^{x_{4}}\\right)^{2}} & - \\frac{e^{x_{0}} e^{x_{2}}}{\\left(e^{x_{0}} + e^{x_{1}} + e^{x_{2}} + e^{x_{3}} + e^{x_{4}}\\right)^{2}} & - \\frac{e^{x_{0}} e^{x_{3}}}{\\left(e^{x_{0}} + e^{x_{1}} + e^{x_{2}} + e^{x_{3}} + e^{x_{4}}\\right)^{2}} & - \\frac{e^{x_{0}} e^{x_{4}}}{\\left(e^{x_{0}} + e^{x_{1}} + e^{x_{2}} + e^{x_{3}} + e^{x_{4}}\\right)^{2}}\\\\- \\frac{e^{x_{0}} e^{x_{1}}}{\\left(e^{x_{0}} + e^{x_{1}} + e^{x_{2}} + e^{x_{3}} + e^{x_{4}}\\right)^{2}} & \\frac{e^{x_{1}}}{e^{x_{0}} + e^{x_{1}} + e^{x_{2}} + e^{x_{3}} + e^{x_{4}}} - \\frac{e^{2 x_{1}}}{\\left(e^{x_{0}} + e^{x_{1}} + e^{x_{2}} + e^{x_{3}} + e^{x_{4}}\\right)^{2}} & - \\frac{e^{x_{1}} e^{x_{2}}}{\\left(e^{x_{0}} + e^{x_{1}} + e^{x_{2}} + e^{x_{3}} + e^{x_{4}}\\right)^{2}} & - \\frac{e^{x_{1}} e^{x_{3}}}{\\left(e^{x_{0}} + e^{x_{1}} + e^{x_{2}} + e^{x_{3}} + e^{x_{4}}\\right)^{2}} & - \\frac{e^{x_{1}} e^{x_{4}}}{\\left(e^{x_{0}} + e^{x_{1}} + e^{x_{2}} + e^{x_{3}} + e^{x_{4}}\\right)^{2}}\\\\- \\frac{e^{x_{0}} e^{x_{2}}}{\\left(e^{x_{0}} + e^{x_{1}} + e^{x_{2}} + e^{x_{3}} + e^{x_{4}}\\right)^{2}} & - \\frac{e^{x_{1}} e^{x_{2}}}{\\left(e^{x_{0}} + e^{x_{1}} + e^{x_{2}} + e^{x_{3}} + e^{x_{4}}\\right)^{2}} & \\frac{e^{x_{2}}}{e^{x_{0}} + e^{x_{1}} + e^{x_{2}} + e^{x_{3}} + e^{x_{4}}} - \\frac{e^{2 x_{2}}}{\\left(e^{x_{0}} + e^{x_{1}} + e^{x_{2}} + e^{x_{3}} + e^{x_{4}}\\right)^{2}} & - \\frac{e^{x_{2}} e^{x_{3}}}{\\left(e^{x_{0}} + e^{x_{1}} + e^{x_{2}} + e^{x_{3}} + e^{x_{4}}\\right)^{2}} & - \\frac{e^{x_{2}} e^{x_{4}}}{\\left(e^{x_{0}} + e^{x_{1}} + e^{x_{2}} + e^{x_{3}} + e^{x_{4}}\\right)^{2}}\\\\- \\frac{e^{x_{0}} e^{x_{3}}}{\\left(e^{x_{0}} + e^{x_{1}} + e^{x_{2}} + e^{x_{3}} + e^{x_{4}}\\right)^{2}} & - \\frac{e^{x_{1}} e^{x_{3}}}{\\left(e^{x_{0}} + e^{x_{1}} + e^{x_{2}} + e^{x_{3}} + e^{x_{4}}\\right)^{2}} & - \\frac{e^{x_{2}} e^{x_{3}}}{\\left(e^{x_{0}} + e^{x_{1}} + e^{x_{2}} + e^{x_{3}} + e^{x_{4}}\\right)^{2}} & \\frac{e^{x_{3}}}{e^{x_{0}} + e^{x_{1}} + e^{x_{2}} + e^{x_{3}} + e^{x_{4}}} - \\frac{e^{2 x_{3}}}{\\left(e^{x_{0}} + e^{x_{1}} + e^{x_{2}} + e^{x_{3}} + e^{x_{4}}\\right)^{2}} & - \\frac{e^{x_{3}} e^{x_{4}}}{\\left(e^{x_{0}} + e^{x_{1}} + e^{x_{2}} + e^{x_{3}} + e^{x_{4}}\\right)^{2}}\\\\- \\frac{e^{x_{0}} e^{x_{4}}}{\\left(e^{x_{0}} + e^{x_{1}} + e^{x_{2}} + e^{x_{3}} + e^{x_{4}}\\right)^{2}} & - \\frac{e^{x_{1}} e^{x_{4}}}{\\left(e^{x_{0}} + e^{x_{1}} + e^{x_{2}} + e^{x_{3}} + e^{x_{4}}\\right)^{2}} & - \\frac{e^{x_{2}} e^{x_{4}}}{\\left(e^{x_{0}} + e^{x_{1}} + e^{x_{2}} + e^{x_{3}} + e^{x_{4}}\\right)^{2}} & - \\frac{e^{x_{3}} e^{x_{4}}}{\\left(e^{x_{0}} + e^{x_{1}} + e^{x_{2}} + e^{x_{3}} + e^{x_{4}}\\right)^{2}} & \\frac{e^{x_{4}}}{e^{x_{0}} + e^{x_{1}} + e^{x_{2}} + e^{x_{3}} + e^{x_{4}}} - \\frac{e^{2 x_{4}}}{\\left(e^{x_{0}} + e^{x_{1}} + e^{x_{2}} + e^{x_{3}} + e^{x_{4}}\\right)^{2}}\\end{matrix}\\right]$"
      ],
      "text/plain": [
       "Matrix([\n",
       "[exp(x_0)/(exp(x_0) + exp(x_1) + exp(x_2) + exp(x_3) + exp(x_4)) - exp(2*x_0)/(exp(x_0) + exp(x_1) + exp(x_2) + exp(x_3) + exp(x_4))**2,                                                           -exp(x_0)*exp(x_1)/(exp(x_0) + exp(x_1) + exp(x_2) + exp(x_3) + exp(x_4))**2,                                                           -exp(x_0)*exp(x_2)/(exp(x_0) + exp(x_1) + exp(x_2) + exp(x_3) + exp(x_4))**2,                                                           -exp(x_0)*exp(x_3)/(exp(x_0) + exp(x_1) + exp(x_2) + exp(x_3) + exp(x_4))**2,                                                           -exp(x_0)*exp(x_4)/(exp(x_0) + exp(x_1) + exp(x_2) + exp(x_3) + exp(x_4))**2],\n",
       "[                                                          -exp(x_0)*exp(x_1)/(exp(x_0) + exp(x_1) + exp(x_2) + exp(x_3) + exp(x_4))**2, exp(x_1)/(exp(x_0) + exp(x_1) + exp(x_2) + exp(x_3) + exp(x_4)) - exp(2*x_1)/(exp(x_0) + exp(x_1) + exp(x_2) + exp(x_3) + exp(x_4))**2,                                                           -exp(x_1)*exp(x_2)/(exp(x_0) + exp(x_1) + exp(x_2) + exp(x_3) + exp(x_4))**2,                                                           -exp(x_1)*exp(x_3)/(exp(x_0) + exp(x_1) + exp(x_2) + exp(x_3) + exp(x_4))**2,                                                           -exp(x_1)*exp(x_4)/(exp(x_0) + exp(x_1) + exp(x_2) + exp(x_3) + exp(x_4))**2],\n",
       "[                                                          -exp(x_0)*exp(x_2)/(exp(x_0) + exp(x_1) + exp(x_2) + exp(x_3) + exp(x_4))**2,                                                           -exp(x_1)*exp(x_2)/(exp(x_0) + exp(x_1) + exp(x_2) + exp(x_3) + exp(x_4))**2, exp(x_2)/(exp(x_0) + exp(x_1) + exp(x_2) + exp(x_3) + exp(x_4)) - exp(2*x_2)/(exp(x_0) + exp(x_1) + exp(x_2) + exp(x_3) + exp(x_4))**2,                                                           -exp(x_2)*exp(x_3)/(exp(x_0) + exp(x_1) + exp(x_2) + exp(x_3) + exp(x_4))**2,                                                           -exp(x_2)*exp(x_4)/(exp(x_0) + exp(x_1) + exp(x_2) + exp(x_3) + exp(x_4))**2],\n",
       "[                                                          -exp(x_0)*exp(x_3)/(exp(x_0) + exp(x_1) + exp(x_2) + exp(x_3) + exp(x_4))**2,                                                           -exp(x_1)*exp(x_3)/(exp(x_0) + exp(x_1) + exp(x_2) + exp(x_3) + exp(x_4))**2,                                                           -exp(x_2)*exp(x_3)/(exp(x_0) + exp(x_1) + exp(x_2) + exp(x_3) + exp(x_4))**2, exp(x_3)/(exp(x_0) + exp(x_1) + exp(x_2) + exp(x_3) + exp(x_4)) - exp(2*x_3)/(exp(x_0) + exp(x_1) + exp(x_2) + exp(x_3) + exp(x_4))**2,                                                           -exp(x_3)*exp(x_4)/(exp(x_0) + exp(x_1) + exp(x_2) + exp(x_3) + exp(x_4))**2],\n",
       "[                                                          -exp(x_0)*exp(x_4)/(exp(x_0) + exp(x_1) + exp(x_2) + exp(x_3) + exp(x_4))**2,                                                           -exp(x_1)*exp(x_4)/(exp(x_0) + exp(x_1) + exp(x_2) + exp(x_3) + exp(x_4))**2,                                                           -exp(x_2)*exp(x_4)/(exp(x_0) + exp(x_1) + exp(x_2) + exp(x_3) + exp(x_4))**2,                                                           -exp(x_3)*exp(x_4)/(exp(x_0) + exp(x_1) + exp(x_2) + exp(x_3) + exp(x_4))**2, exp(x_4)/(exp(x_0) + exp(x_1) + exp(x_2) + exp(x_3) + exp(x_4)) - exp(2*x_4)/(exp(x_0) + exp(x_1) + exp(x_2) + exp(x_3) + exp(x_4))**2]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax_jac = softmax.jacobian(x)\n",
    "softmax_jac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax_lambda = sp.lambdify(x, softmax)\n",
    "jacobian_lambda = sp.lambdify(x, softmax_jac)\n",
    "\n",
    "y_value_sp = softmax_lambda(*x_input).ravel()\n",
    "dL_dx_sp = (jacobian_lambda(*x_input) @ dL_dy).ravel()\n",
    "\n",
    "assert ((y_value_sp - y_value)<1e-3).all()\n",
    "assert ((dL_dx_sp - dL_dx)<1e-3).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, `SoftMax` supports matrix inputs. It will apply `SoftMax` rowwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm = SoftMax((2, 5))\n",
    "\n",
    "# Random input values (x itself, and assumed partial derivative)\n",
    "x_input = np.array([\n",
    "    [0, 1, 2, 3, 4,],\n",
    "    [0, 1, 2, 3, 4]\n",
    "])\n",
    "dL_dy = np.array([\n",
    "    [1, 2, 3, 2, 1], \n",
    "    [1, 2, 3, 2, 1]\n",
    "])\n",
    "\n",
    "# Forward call\n",
    "y_value = sm.forward(x_input)\n",
    "\n",
    "# Backpropogation\n",
    "dL_dx = sm.backward(dL_dy)\n",
    "\n",
    "for i in range(2):\n",
    "    assert ((y_value_sp - y_value[i])<1e-3).all()\n",
    "    assert ((dL_dx_sp - dL_dx[i])<1e-3).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2. Normalized SoftMax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The forward pass of normalized SoftMax is trivial to implement. Regarding jacobian, it could be expressed as follows:\n",
    "$$\\frac{\\partial y}{\\partial x} = \\frac{\\partial y}{\\partial x^{norm}}\\frac{\\partial x^{norm}}{\\partial x}$$\n",
    "\n",
    "where term $\\frac{\\partial y}{\\partial x^{norm}}$ is a softmax function of $x^{norm}$ term, and $\\frac{\\partial x^{norm}}{\\partial x}$ structure depends on the maximal element idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One could see, that the only differences are:\n",
    "1. Denominator everywhere is multiplied by a maximal element\n",
    "2. The differentiation over maximal element contains contains an additional term\n",
    "3. The diagonal element, corresponding to the maximal element contains additional term too"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using that, it's easy to compute jacobian numerically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.11405072, 0.14644403, 0.31002201, 0.18803785, 0.24144538]),\n",
       " array([ 0.02596599,  0.04249374, -0.00692263,  0.01930594, -0.08084303]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm = NormalizedSoftMax(5)\n",
    "\n",
    "# Random input values (x itself, and assumed partial derivative)\n",
    "x_input = np.array([0, 1, 4, 2, 3])\n",
    "dL_dy = np.array([1, 2, 3, 2, 1])\n",
    "\n",
    "# Forward call\n",
    "y_value = sm.forward(x_input)\n",
    "\n",
    "# Backpropogation\n",
    "dL_dx = sm.backward(dL_dy, reset_after=False)\n",
    "\n",
    "y_value, dL_dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To validate the result, let's use the `sympy` for a numerical calculations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 5\n",
    "x = sp.symbols(' '.join([f'x_{i}' for i in range(n)]))\n",
    "x_norm = sp.Matrix([x_i / x[2] for x_i in x])\n",
    "exp_x_norm = sp.Matrix([\n",
    "    sp.exp(x_norm_i) for x_norm_i in x_norm\n",
    "])\n",
    "exp_sum_norm = sum(exp_x_norm)\n",
    "\n",
    "# Assume that 2th element is the biggest\n",
    "softmax_norm = exp_x_norm / exp_sum_norm\n",
    "softmax_norm_jac = softmax_norm.jacobian(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax_lambda = sp.lambdify(x, softmax_norm)\n",
    "jacobian_lambda = sp.lambdify(x, softmax_norm_jac)\n",
    "\n",
    "y_value_sp = softmax_lambda(*x_input).ravel()\n",
    "dL_dx_sp = (jacobian_lambda(*x_input) @ dL_dy).ravel()\n",
    "\n",
    "assert ((y_value_sp - y_value)<1e-3).all\n",
    "assert ((dL_dx_sp - dL_dx)<1e-3).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, `NormalizedSoftMax` supports matrix inputs. It will apply `NormalizedSoftMax` rowwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm = NormalizedSoftMax((2, 5))\n",
    "\n",
    "# Random input values (x itself, and assumed partial derivative)\n",
    "x_input = np.array([\n",
    "    [0, 1, 4, 2, 3],\n",
    "    [0, 1, 4, 2, 3]\n",
    "])\n",
    "dL_dy = np.array([\n",
    "    [1, 2, 3, 2, 1], \n",
    "    [1, 2, 3, 2, 1]\n",
    "])\n",
    "\n",
    "# Forward call\n",
    "y_value = sm.forward(x_input)\n",
    "\n",
    "# Backpropogation\n",
    "dL_dx = sm.backward(dL_dy)\n",
    "\n",
    "for i in range(2):\n",
    "    assert ((y_value_sp - y_value[i])<1e-3).all()\n",
    "    assert ((dL_dx_sp - dL_dx[i])<1e-3).all()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_courses",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
