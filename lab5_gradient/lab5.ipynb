{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 5. Vector and Matrix Backpropogation\n",
    "by Domrachev Ivan, B20-Ro-01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nn_from_scratch.nodes import SoftMax, ReLU\n",
    "from nn_from_scratch.neurons import Linear\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1. Vectorized softmax function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note: accidentally, I've implemented this functionality in the previous lab assignment. Here is the copy of corresponding charpter from there"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the logic is implemented in the file [backprop.py](./backprop.py). There is an abstract class Node, which implements all the expected logic except for the function itself and the jacobian matrix calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.01165623, 0.03168492, 0.08612854, 0.23412166, 0.63640865]),\n",
       " array([-0.00510617,  0.01780491,  0.1345273 ,  0.13156147, -0.27878751]))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm = SoftMax(5)\n",
    "\n",
    "# Random input values (x itself, and assumed partial derivative)\n",
    "x_input = np.array([0, 1, 2, 3, 4])\n",
    "dL_dy = np.array([1, 2, 3, 2, 1])\n",
    "\n",
    "# Forward call\n",
    "y_value = sm.forward(x_input)\n",
    "\n",
    "# Backpropogation\n",
    "dL_dx = sm.backward(dL_dy)\n",
    "\n",
    "y_value, dL_dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The verification was described in the previous lab, the code (as well as outputs) haven't changed, so let's omit that here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2. ReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function ReLU, as well as the partial derivative, were implemented according to the lecture. Note that despite derivative $\\frac{\\partial ReLU}{\\partial x} (0)$ was chosen, it was assigned to $0$ in my implementation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 0, 2, 0, 4]), array([0., 0., 3., 0., 1.]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relu = ReLU(5)\n",
    "\n",
    "# Random input values (x itself, and assumed partial derivative)\n",
    "x_input = np.array([0, -1, 2, -1, 4])\n",
    "dL_dy = np.array([1, 2, 3, 2, 1])\n",
    "\n",
    "# Forward call\n",
    "y_value = relu.forward(x_input)\n",
    "\n",
    "# Backpropogation\n",
    "dL_dx = relu.backward(dL_dy)\n",
    "\n",
    "y_value, dL_dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is easy to see from that example that `ReLU` function works correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3. Linear layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To implement linear node $Y = WX$ in my structure, I decided to make the following assumptions:\n",
    "1. The weigts matrix `W` is an inner parameter of the class, which could be accessed via a getter. It's partial derivative is also stored inside it.\n",
    "2. Despite there are functions to compute derivatives w.r.t. `W` and `X`, I decided to override the functoin `forward` with dot products from `numpy`, because this approach is more efficient\n",
    "\n",
    "You can explore the implementation in `backprop.py` file.\n",
    "> *Note. To compute the jacobian $\\frac{\\partial Y}{\\partial X}$ and $\\frac{\\partial Y}{\\partial W}$, the key note is that:*\n",
    "> 1. *$\\frac{\\partial Y}{\\partial X}_{i, j}$ is a zero matrix, except for the $j$-th column, which equals to:*\n",
    "> $$\\frac{\\partial Y}{\\partial X}_{i, j, l, j} = W_{l, j}$$\n",
    "> 2. *Similarly, $\\frac{\\partial Y}{\\partial W}_{i, j}$ is a zero matrix, except for the $i$-th raw, which equals to:*\n",
    "> $$\\frac{\\partial Y}{\\partial W}_{i, j, j, l} = X_{j, l}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward output:\n",
      "[[0.69166497 1.05687395 0.66806232]\n",
      " [0.66735337 0.96454763 0.65531636]\n",
      " [0.75730103 1.06981767 0.73763436]\n",
      " [0.68926678 1.05670438 0.67034384]\n",
      " [0.74321974 1.12798567 0.72313718]]\n",
      "Jacobian dL/dx:\n",
      "[[1.89233349 0.84690493 0.92517105]\n",
      " [1.81512837 0.78568075 0.86684344]]\n",
      "Jacobian dL/dw:\n",
      "[[1.49979587 0.37961463 0.43178553]\n",
      " [1.0411152  0.20627368 0.24631968]\n",
      " [2.09897645 0.47136409 0.63882777]\n",
      " [1.38170136 0.16064022 0.31876463]\n",
      " [0.98934576 0.31704245 0.34003937]]\n"
     ]
    }
   ],
   "source": [
    "input_dim=(2, 3)\n",
    "output_dim=(5, 3)\n",
    "relu = Linear(input_dim, output_dim)\n",
    "\n",
    "# Random input values (x itself, and assumed partial derivative)\n",
    "x_input = np.random.rand(*input_dim)\n",
    "dL_dy = np.random.rand(*output_dim)\n",
    "\n",
    "# Forward call\n",
    "y_value = relu.forward(x_input)\n",
    "\n",
    "# Backpropogation\n",
    "dL_dx = relu.backward(dL_dy)\n",
    "dL_dw = relu._W_pd\n",
    "\n",
    "print(\"Forward output:\")\n",
    "print(y_value)\n",
    "print(\"Jacobian dL/dx:\")\n",
    "print(dL_dx)\n",
    "print(\"Jacobian dL/dw:\")\n",
    "print(dL_dw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also note that calculations of back propogation wia jacobians lead to the same result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jacobian dL/dx:\n",
      "[[1.89233349 0.84690493 0.92517105]\n",
      " [1.81512837 0.78568075 0.86684344]]\n",
      "Jacobian dL/dw\n",
      "[[1.49979587 0.37961463 0.43178553]\n",
      " [1.0411152  0.20627368 0.24631968]\n",
      " [2.09897645 0.47136409 0.63882777]\n",
      " [1.38170136 0.16064022 0.31876463]\n",
      " [0.98934576 0.31704245 0.34003937]]\n"
     ]
    }
   ],
   "source": [
    "dL_dx_jac = relu.backward(dL_dy, use_jacobian=True)\n",
    "dL_dw_jac = relu._W_pd\n",
    "\n",
    "print(\"Jacobian dL/dx:\")\n",
    "print(dL_dx_jac)\n",
    "\n",
    "print(\"Jacobian dL/dw\")\n",
    "print(dL_dw_jac)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_courses",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
